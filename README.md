# data-collection
# Background : The assignment is split into two parts where I will scrape titles and preview text from Mars news articles. The second part consists of scraping and analyzing Mars weather data. 

# Solution : In order to complete this assignment, I used my instructors code for refernce https://git.bootcampcontent.com/Northwestern-University/NU-VIRT-DATA-PT-06-2024-U-LOLC.git
# Part 1- Scrape Titles and Preview Text: Importing necessary dependencies to complete the assignment. I used automated browsing to visit the Mars news site. I then created a Beautiful Soup object to extract text elements from the website. Next, I extracted the titles and preview texts of news articles that I scraped, and stored the scraping results into Python dictionaries. I stored the dictionaries in a Python list and printed the list in my notebook. 

# Part 2- Scrape and Analyze Mars Weather Data: I began by importing my libraries to run my code. I used the automated browing to visit the Mars Temperature Data Site and identify which elements to scrape. Next, I created a Beautiful Soup object to scrape the data in the HTML table. I organized the scraped data into a Pandas DataFrame, with columns that have the same headings as the website. I then examined the data types that were actively associated with each column. I changed the data types for data analysis. Using the dataset I found how many months exist on Mars, how many Martian days of data there are, what the average minimum temperature is by month. After that, I plotted the average minimum temperature by month using a bar graph, identified the coldest and hottest months in Curiosity's location by sorting the previous graph. I found the average pressure by month and displayed it using a bar graph, then identifying the lowest and highest pressure months in Curiosity's location by sorting the previous graph and creating a new one. I found how many Earth days ther are in a Martian year. To do this, I had to visually estimate the result by plotting the daily minimum temperature of each observation in the data set. Finally, writing the data into a CSV file. 
